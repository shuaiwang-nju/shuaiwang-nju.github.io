---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<!-- {% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %} -->

<span class='anchor' id='about-me'></span>


Dr. Shuai Wang is currently a Tenure-Track Associate Professor at the School of Intelligence Science and Technology, Nanjing University. He earned his B.E. degree from Northwestern Polytechnical University in 2014 under the supervision of [Prof. Lei Xie](http://lxie.npu-aslp.org/), and his Ph.D. degree from Shanghai Jiao Tong University in 2020 under the supervision of [Prof. Kai Yu](https://x-lance.github.io/kaiyu/) and [Prof. Yanmin Qian](https://audiocc.sjtu.edu.cn/en/members/yanmin.qian). Prior to joining Nanjing University, he served as a research scientist in [Prof. Haizhou Li](https://www.colips.org/~eleliha/)'s team at the Shenzhen Research Institute of Big Data, Chinese University of Hong Kong (Shenzhen), where he still holds an adjunct position now. Additionally, he spent 2.5 years as a senior research scientist at Lightspeed & Quantum Studios, Tencent, where he led the speech group in R&D of speech technologies customized for games.

His research interest includes speaker modeling,target speaker processing,  speech synthesis, voice conversion and music generation. He has published more than 60 papers at top-tier speech conferences/journals. 

<!-- <a href='https://scholar.google.com/citations?user=vW1ZaucAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a> -->


# ğŸ”¥ Openings
I will have several openings for graduate students (2026 Fall), will update details ASAP. I am currently looking for research assistants, please feel free to drop me an email with your CV if you are interested in the following topics:

- Speaker Modeling
- Target Speaker Processing
- Speech Generation
- Music Generation
- Brain-inspired speech processing

> Note that research assistants can choose to work either in Nanjing University @ Suzhou or in Chinese University of Hong Kong (Shenzhen), jointly supervised with Prof. [Haizhou Li](https://www.colips.org/~eleliha/).

---

<span class='anchor' id='-recruitment'></span>

## ğŸ“ **æ‹›ç”Ÿä¿¡æ¯ / Recruitment Information**



<!-- Video Section -->
<div class="video-container" style="text-align: center; margin: 20px 0;">
  <p style="font-size: 14px; color: #666; margin-bottom: 10px; font-style: italic;">
    ğŸµ åŸºäºæˆ‘ä»¬è®ºæ–‡ <a href="https://arxiv.org/abs/2506.07634" target="_blank" style="color: #007acc; text-decoration: none;">SongBloom</a> ç”Ÿæˆçš„æ‹›ç”Ÿæ¬¢è¿æ¼”ç¤º / Welcome demo generated using our <a href="https://arxiv.org/abs/2506.07634" target="_blank" style="color: #007acc; text-decoration: none;">SongBloom</a> paper
  </p>
  <video width="50%" max-width="500px" controls style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
    <source src="{{ site.baseurl }}/images/zhike_song.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
</div>

<div class="recruitment-section">

<h3 class="recruitment-title">ğŸ”¥ 2026å¹´ç§‹å­£ç ”ç©¶ç”Ÿæ‹›ç”Ÿ / 2026 Fall Graduate Student Recruitment</h3>

<p class="recruitment-subtitle">æ‹›ç”Ÿæ–¹å‘ / Research Areas:</p>
<ul class="recruitment-list">
<li><strong>è¯´è¯äººå»ºæ¨¡ / Speaker Modeling</strong></li>
<li><strong>ç›®æ ‡è¯´è¯äººå¤„ç† / Target Speaker Processing</strong></li>
<li><strong>è¯­éŸ³ç”Ÿæˆ / Speech Generation</strong></li>
<li><strong>éŸ³ä¹ç”Ÿæˆ / Music Generation</strong></li>
<li><strong>ç±»è„‘è¯­éŸ³å¤„ç† / Brain-inspired Speech Processing</strong></li>
</ul>

<p class="recruitment-subtitle">ç”³è¯·è¦æ±‚ / Requirements:</p>
<ul class="recruitment-list">
<li>è®¡ç®—æœºç§‘å­¦ã€ç”µå­å·¥ç¨‹æˆ–ç›¸å…³ä¸“ä¸šèƒŒæ™¯ / Background in Computer Science, Electronic Engineering, or related fields</li>
<li>å¯¹è¯­éŸ³å¤„ç†ã€æœºå™¨å­¦ä¹ æœ‰æµ“åšå…´è¶£ / Strong interest in speech processing and machine learning</li>
<li>è‰¯å¥½çš„ç¼–ç¨‹èƒ½åŠ›ï¼ˆPython/C++ï¼‰ / Good programming skills (Python/C++)</li>
<li>è‹±è¯­è¯»å†™èƒ½åŠ›è‰¯å¥½ / Good English reading and writing skills</li>
</ul>

<p class="recruitment-subtitle">è”ç³»æ–¹å¼ / Contact:</p>
<ul class="recruitment-list">
<li>é‚®ç®± / Email: <span id="email-display" class="recruitment-email" style="cursor: pointer;" onclick="showEmail()">ç‚¹å‡»æ˜¾ç¤ºé‚®ç®± / Click to show email</span></li>
<li>è¯·é™„ä¸Šç®€å†ã€æˆç»©å•å’Œç ”ç©¶å…´è¶£é™ˆè¿° / Please include CV, transcripts, and research interest statement</li>
</ul>

<p class="recruitment-subtitle"><strong>å—å¤§æ™ºç§‘å­¦ç”Ÿç‰¹åˆ«è¯´æ˜ / Special Notice for NJU Students:</strong></p>
<ul class="recruitment-list">
<li>æ¬¢è¿å¤§äºŒå¤§ä¸‰å­¦ç”Ÿè¿›ç»„å®ä¹  / Welcome sophomore and junior students for internships</li>
<li class="office-highlight">å—å¤§æ™ºç§‘å­¦ç”Ÿå¯åˆ°å—é›æ¥¼è¥¿536åŠå…¬å®¤é¢èŠ / NJU students can drop by Room 536 at Nanyong Building for face-to-face discussion</li>
<li>å®ä¹ æœŸé—´å¯å‚ä¸å®é™…ç§‘ç ”é¡¹ç›® / Interns can participate in actual research projects</li>
</ul>

</div>

# ğŸ‘¨â€ğŸ“ Students

Ph.D. students jointly supervised with Prof. [Haizhou Li](https://www.colips.org/~eleliha/)

- Chenyu Yang, CUHK-Shenzhen, Music Generation, Intern at Tencent AILab ï¼ˆçŠ€ç‰›é¸Ÿäººæ‰è®¡åˆ’ï¼‰.
- [Zhijun Liu](https://zjlww.github.io/), CUHK-Shenzhen, Speech Synthesis, Intern at NetEase and Bytedance (TopSeed)
- [Sho Inoue](https://www.linkedin.com/in/sho-inoue-41646a1a2/), CUHK-Shenzhen, Speech Syntesis, Intern at NetEase and Meta FAIR.
- [Qibing Bai](https://p1ping.github.io/), CUHK-Shenzhen, Accent Conversion, Intern at Tencent TEA-Lab
- Wenxuan Wu, CUHK, Target Speech Extraction
- Wupeng Wang, NUS, Speech Separation

## Past students

- [Junjie Li](https://mrjunjieli.github.io/), currently Ph.D. student at The Hong Kong Polytechnic University



# ğŸ“ Publications 

Please check my [Google Scholar](https://scholar.google.com/citations?user=vW1ZaucAAAAJ) for the latest publications.

# ğŸªœ Open-Source Projects

- [WeSpeaker](https://github.com/wenet-e2e/wespeaker): Speaker Embedding Learning
- [WeSep](https://github.com/wenet-e2e/wesep): Target Speaker Extraction.
- [DiffRhythm](https://github.com/ASLP-lab/DiffRhythm): Diffusion-based Rhythmic Music Generation.


# ğŸ– Honors and Awards
- *2024* Best Paper Award, ISCSLP 2024
- *2024* Best Student Paper Award, ISCSLP 2024
- *2019* VoxSRC 2019:	Rank 1st in both 2 Tracks
- *2019* DIHARD 2019: Rank 1st in both 4 Tracks
- *2018* IEEE Ganesh N. Ramaswamy Memorial Student Grant Award

# ğŸŒ… Services
I serve as a regular reviewer for multiple conferences and journals, including 
- ICASSP, Interspeech, ASRU, SLT, T-ASLP, Computer Speech & Language, Speech Communication; 
- ICML, Neurips, AAAI, ACM MM.

I serve as the Specical Session Chair of APSIPA 2025, the Operation Chair of ICASSP 2025 Suzhou Satellite Event, the Publication Chair of SLT 2024.


# ğŸ’¬ Invited Talks
- *2024.09*, [Speaker Representation Learning: Theories, Applications and Practice](https://vgs-it.fit.vutbr.cz/2024/09/03/shuai-wang-speaker-representation-learning-theories-applications-and-practiceshuai-wang/) at Brno University of Technology. [\[video\]](https://www.youtube.com/live/FMY5_smgrYY)

